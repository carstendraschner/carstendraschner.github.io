<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Carsten Felix Draschner - AI R&amp;D</title>
  <subtitle>Various updates and insides into cutting edge AI Research and Development including AI Ethics and perspective from European Union.</subtitle>
  <link href="https://carstendraschner.github.io/feed/feed.xml" rel="self" />
  <link href="https://carstendraschner.github.io/blog/" />
  <updated>2024-12-27T00:00:00Z</updated>
  <id>https://carstendraschner.github.io/blog/</id>
  <author>
    <name>Carsten Felix Draschner</name>
  </author>
  <entry>
    <title>New DE/EU Open Source LLM 🇪🇺 Teuken 7B, Now Truly Open Source?</title>
    <link href="https://carstendraschner.github.io/blog/241127_Teuken_Review/" />
    <updated>2024-12-27T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241127_Teuken_Review/</id>
    <content type="html">&lt;p&gt;New Model out of Open-GPT-X developed within Germany from European Perspective&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/u966D37VBY-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/u966D37VBY-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/u966D37VBY-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;749&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New German OS LLM&lt;/li&gt;
&lt;li&gt;7B, Transperant Docs&lt;/li&gt;
&lt;li&gt;Problems in Alignment?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;short-facts&quot;&gt;Short Facts&lt;/h2&gt;
&lt;p&gt;🇪🇺 European language focus
🤖 7B parameters
🔗 Very open Apache 2.0 license
🛠️ Instruction Fine-tuned
📊 Not outperforming in Benchmarks but also within normal distribution (for 🇬🇧)
👶🏼 4k max context
💣 (EDIT) Alignment Discussion see here: &lt;a href=&quot;https://lnkd.in/eseXDMMS&quot;&gt;https://lnkd.in/eseXDMMS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My former colleagues Mehdi Ali (roommate within our time in Smart Data Analytics at Rheinische Friedrich-Wilhelms-Universität Bonn of Jens Lehmann) and Stefan Wrobel (one of my Ph.D. supervisors Rheinische Friedrich-Wilhelms-Universität Bonn) and their teams have released an LLM developed in Europe within OpenGPT-X &amp;amp; Fraunhofer IAIS. This has been developed with a focus on European languages. I&#39;m excited to try it out right away and see the actual performance! They announced their own Leaderboard and the Model in somewhere in between other models of same size: &lt;a href=&quot;https://lnkd.in/eTgv8bjV&quot;&gt;https://lnkd.in/eTgv8bjV&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;my-take&quot;&gt;My Take 🤗&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I have high hopes for a better documentation of the development process and I am curious if this model can be called truly open source and not just open weight.&lt;/li&gt;
&lt;li&gt;I look forward to further European and German initiatives that also pursue (Gen)AI development here in 🇪🇺❤️&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;my-questions-at-mehdi-ali-michael-fromm-max-luebbering-phd-and-all-your-contributors&quot;&gt;My Questions (at Mehdi Ali, Michael Fromm, Max Lübbering, PhD and all your contributors):&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Is anyone capable to entirely reproduce the model from the documentation?&lt;/li&gt;
&lt;li&gt;What do you expect from Arena Elo Scores?&lt;/li&gt;
&lt;li&gt;In which tasks would you recommend usage of your model over other models from Mistral AI, Meta, Google, Microsoft, Alibaba Cloud like Mistral7B, LLama3.18B, Gemma, Phi, Qwen...&lt;/li&gt;
&lt;li&gt;Will we see truly big LLMs in size of 50-400B Parameter that can compete with current top notch LLMs from your side?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We at Comma Soft AG provide with Alan.de a European and German Solution to use GenAI in a highly performant, safe and trusted environment. If you like to see more of our recent R&amp;amp;D, check my recent posts and reach out to me for more idea exchange 😊&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>We&#39;re Dealing with the Butterfly Effect in LLM Creation Pipelines 🦋🤖</title>
    <link href="https://carstendraschner.github.io/blog/241120_were_dealing_with_the_butterfly_effect_in_llm_creation_pipelines/" />
    <updated>2024-11-20T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241120_were_dealing_with_the_butterfly_effect_in_llm_creation_pipelines/</id>
    <content type="html">&lt;p&gt;When we develop novel LLMs, AI-Agents and GenAI Pipelines for Alan.de or within our various other GenAI projects, we&#39;re continuously learning about the Butterfly Effect in GenAI creation pipelines and how to mitigate this problem. 👨🏼‍💻
🤯 Every change can have a huge impact on the entire pipeline that creates and executes models&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/ODUjemRpOm-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/ODUjemRpOm-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/ODUjemRpOm-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Butterfly Effect can significantly impact LLM creation pipelines.&lt;/li&gt;
&lt;li&gt;Small changes in preprocessing or generation configs can have large ripple effects.&lt;/li&gt;
&lt;li&gt;Constantly evaluating trade-offs and staying up-to-date with new models is crucial.&lt;/li&gt;
&lt;li&gt;Strategies to mitigate the Butterfly Effect are essential for optimal performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-butterfly-effect-in-action&quot;&gt;The Butterfly Effect in action:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📃 &lt;strong&gt;Example 1:&lt;/strong&gt; Preprocessing differences in raw-document data extraction can result in different extracted text, facts, training and benchmarking data, and ultimately, different model performance and benchmark results.&lt;/li&gt;
&lt;li&gt;📊 &lt;strong&gt;Example 2:&lt;/strong&gt; Even a slight change in the generation config (e.g. top_p) of a Benchmarking-Data generating AI Agent can have a ripple effect on the entire pipeline to the finally selected best LLM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-challenge&quot;&gt;The Challenge:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👩🏻‍🔬 With new models and agents (e.g. LLM-as-Judge) becoming available every day, we need to be prepared to exchange components frequently to stay up-to-date and ensure optimal performance and reliability of our pipeline&lt;/li&gt;
&lt;li&gt;⚖️ This means constantly evaluating the trade-offs between performance, reliability, and interpretability, and making informed decisions about when to update or replace components.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;my-takes&quot;&gt;My Takes:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🧐 Be aware of the Butterfly Effect in your LLM creation pipelines and take steps to mitigate its impact.&lt;/li&gt;
&lt;li&gt;👨🏼‍🎓 Stay up-to-date with the latest developments (insane task) in the field and be prepared to adapt and evolve your solutions accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;my-questions&quot;&gt;My Questions:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;🦋 How do you handle the Butterfly Effect in your LLM creation pipelines?&lt;/li&gt;
&lt;li&gt;🗞️ What strategies do you use to stay up-to-date with the latest developments in the field and ensure optimal performance?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In recent and especially upcoming posts I will address how we Comma Soft AG handle this challenge. If you like to see more of the details please leave me a comment what you think and which questions and ideas you have 💬❤️&lt;/p&gt;
&lt;p&gt;#generativeai #artificialintelligence #llm #machinelearning #alan&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>New Official OSI &quot;Open Source AI definition&quot; attacks Meta&#39;s LLAMA as &quot;Open Washing&quot;</title>
    <link href="https://carstendraschner.github.io/blog/241113_new_official_osi_open_source_ai_definition_attacks_metas_llama_as_open_washing/" />
    <updated>2024-11-13T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241113_new_official_osi_open_source_ai_definition_attacks_metas_llama_as_open_washing/</id>
    <content type="html">&lt;p&gt;We getting closer what should be considered being open source in GenAI and not only open weight.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/MZY7DxFWA2-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/MZY7DxFWA2-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/MZY7DxFWA2-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR 📝&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Official OSI definition of Open Source AI available.&lt;/li&gt;
&lt;li&gt;Meta doesn&#39;t like it 🙅‍♂️.&lt;/li&gt;
&lt;li&gt;State-of-the-art (SOTA) models lack being truly Open Source 🤔.&lt;/li&gt;
&lt;li&gt;&amp;quot;Open Washing&amp;quot; Problem of Meta and others?!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;osi-definition&quot;&gt;OSI Definition 📜&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Publicly accessible training data 📊.&lt;/li&gt;
&lt;li&gt;Publicly accessible code for creation 💻.&lt;/li&gt;
&lt;li&gt;Transparent model weights ⚖️.&lt;/li&gt;
&lt;li&gt;Permissive licensing for usage and modification 📝.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;my-take&quot;&gt;My Take 🤔&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I mostly agree with the definition of Open Source Initiative (OSI) Open Source AI 👍.&lt;/li&gt;
&lt;li&gt;I hope more SOTA models like LLAMA of Meta would follow this definition when claiming they are Open Source 🤞.&lt;/li&gt;
&lt;li&gt;There is Open Washing out there, either in title (like OpenAI) or in claiming models are open source but not reproducible (training data and sampling and creation pipeline) 🚫.&lt;/li&gt;
&lt;li&gt;I still appreciate that more and more non-fully open source models are available and are at least Open Weight SOTA GenAI models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading 📚&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Problems with Llama &amp;quot;Open Source&amp;quot; License: &lt;a href=&quot;https://lnkd.in/dtStQSdn&quot;&gt;https://lnkd.in/dtStQSdn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What &amp;quot;Open&amp;quot; Should Truly Mean: &lt;a href=&quot;https://lnkd.in/e7vxhzKi&quot;&gt;https://lnkd.in/e7vxhzKi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OSI Definition: &lt;a href=&quot;https://lnkd.in/ePtGNnvh&quot;&gt;https://lnkd.in/ePtGNnvh&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you ❤️ this content, leave me a thumb up 👍, and please share your thoughts in the comments 💬&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #opensource #meta #llama&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>The Uncanny Valley Phenomenon in GenAI Face Synthesis</title>
    <link href="https://carstendraschner.github.io/blog/241105_the_uncanny_valley_phenomenon_in_genai_face_synthesis/" />
    <updated>2024-11-05T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241105_the_uncanny_valley_phenomenon_in_genai_face_synthesis/</id>
    <content type="html">&lt;p&gt;GenAI hits Uncanny Valley Problems as we have seen in Animation and Computer Games&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/MFdV8_8zRS-946.avif 946w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/MFdV8_8zRS-946.webp 946w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/MFdV8_8zRS-946.png&quot; alt=&quot;Image 1&quot; width=&quot;946&quot; height=&quot;870&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR 📝&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Official OSI definition of Open Source AI available.&lt;/li&gt;
&lt;li&gt;Meta doesn&#39;t like it 🙅‍♂️.&lt;/li&gt;
&lt;li&gt;State-of-the-art (SOTA) models lack being truly Open Source 🤔.&lt;/li&gt;
&lt;li&gt;&amp;quot;Open Washing&amp;quot; Problem of Meta and others?!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-is-the-uncanny-valley&quot;&gt;What Is The Uncanny Valley?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The uncanny valley is a phenomenon where human-like objects or characters that are almost, but not quite, indistinguishable from real humans can evoke a sense of eeriness or discomfort. 🤖&lt;/li&gt;
&lt;li&gt;This concept is particularly relevant in the context of generative AI video synthesis of emotional human faces. 📹&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-challenge-of-recreating-lifelike-faces&quot;&gt;The Challenge of Recreating Lifelike Faces&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Creating AI-generated faces that are both realistic and emotionally expressive is a significant challenge. 🤔&lt;/li&gt;
&lt;li&gt;Small imperfections in facial expressions, movements, or emotional nuances can make the synthesized faces appear strange or creepy to viewers. 👻&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-impact-on-emotional-intelligence&quot;&gt;The Impact on Emotional Intelligence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The uncanny valley phenomenon can have a significant impact on the emotional intelligence of AI systems. 🤖&lt;/li&gt;
&lt;li&gt;If AI-generated faces are not convincingly realistic, they may not be able to evoke the desired emotional response from humans. 🤔&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-do-you-think&quot;&gt;What Do You Think?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Can you think of any potential solutions to overcome the uncanny valley phenomenon in generative AI (face synthesis)? 💡&lt;/li&gt;
&lt;li&gt;Do you believe that the uncanny valley is a significant challenge that needs to be addressed in the development of emotionally intelligent AI systems? 🤔&lt;/li&gt;
&lt;li&gt;Share your thoughts and ideas in the comments! 💬&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#genai #deeptech #artificialintelligence #uncannyvalley&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>A GenAI Solution/Tweak to Rescue ARD tagesschau webpage?!</title>
    <link href="https://carstendraschner.github.io/blog/241025_a_genai_solution_tweak_to_rescue_ard_tagesschau_webpage/" />
    <updated>2024-10-25T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241025_a_genai_solution_tweak_to_rescue_ard_tagesschau_webpage/</id>
    <content type="html">&lt;p&gt;Brainstorming how old regulations meet new GenAI opportunities.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/O_cWA3fM8t-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/O_cWA3fM8t-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/O_cWA3fM8t-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR 📝&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Debate over new regulations restricting text content of public broadcasters online.&lt;/li&gt;
&lt;li&gt;Reform could impact news portals like tagesschau.de by limiting text content to only that which accompanies broadcasts.&lt;/li&gt;
&lt;li&gt;GenAI could potentially auto-generate videos from text content to comply with regulations.&lt;/li&gt;
&lt;li&gt;Questions about the impact and regulation of GenAI in public broadcasting contexts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I am wondering if my proposed solution would contradict the possible upcoming regulations (see below) 🤔.&lt;/li&gt;
&lt;li&gt;I might be biased as tagesschau.de is for me one good source of news with a mix of text and video while being free of ads 📰(IMHO).&lt;/li&gt;
&lt;li&gt;I am writing this as only one example where old regulations might clash with or ignore recent GenAI developments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Current Debate:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More or less every household needs to pay ~18€ monthly for public broadcast 💶.&lt;/li&gt;
&lt;li&gt;State premiers are discussing a reform that would significantly restrict the text content of public broadcasters like ARD and ZDF online 📄.&lt;/li&gt;
&lt;li&gt;The draft reform mandates that only texts accompanying broadcasts are permitted, meaning text content can only be published if it has been previously covered in a TV broadcast 📺.&lt;/li&gt;
&lt;li&gt;This change would greatly impact the speed and diversity of reporting, particularly for news portals like tagesschau.de, they say 🚫.&lt;/li&gt;
&lt;li&gt;Private publishers, viewing this as a competitive advantage, support the reform, while public broadcaster representatives warn of negative effects on media diversity and information provision 🏢.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;My GenAI Perspective:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What would happen if these platforms autogenerate videos on some digital video stream platform based on the text content they propose? 🤖.&lt;/li&gt;
&lt;li&gt;We have seen such approaches of text-to-video generation or speech synthesis 📝.&lt;/li&gt;
&lt;li&gt;Maybe with a digital clone of some professional or artificial speaker (who needs to allow this for sure and should get paid for giving away the digital twin) 🗣️.&lt;/li&gt;
&lt;li&gt;Currently we face several conflicts of old media and GenAI but this is more between public broadcast and private publishers ⚖️.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do you see it? 🤷.&lt;/li&gt;
&lt;li&gt;Do you think GenAI content would solve their issue? 🤔.&lt;/li&gt;
&lt;li&gt;How is it regulated within your country if public broadcast can write articles? 🌍.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Link to 🇩🇪 Tagesschau article: &lt;a href=&quot;https://lnkd.in/dkx_RZun&quot;&gt;https://lnkd.in/dkx_RZun&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;#Rundfunkbeitrag #genai #gez #presse #artificialintelligence&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>We Cannot Waste Time and Resources through Regenerating GenAI Creations! GenAI Control is Key for Productivity!</title>
    <link href="https://carstendraschner.github.io/blog/241016_we_cannot_waste_time_and_resources_through_regenerating_genai_creations_genai_control_is_key_for_productivity/" />
    <updated>2024-10-16T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241016_we_cannot_waste_time_and_resources_through_regenerating_genai_creations_genai_control_is_key_for_productivity/</id>
    <content type="html">&lt;p&gt;We will see more and more precise adjustment options in GenAI Tooling&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/DGwfP2s1_T-664.avif 664w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/DGwfP2s1_T-664.webp 664w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/DGwfP2s1_T-664.png&quot; alt=&quot;Image 1&quot; width=&quot;664&quot; height=&quot;488&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GenAI often lacks precise Adjuments&lt;/li&gt;
&lt;li&gt;Tools and Models start CLosing the Gap&lt;/li&gt;
&lt;li&gt;Major Waste of Time and Resources through inefficient Regenerates&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;control-and-hallucinations-in-generated-videos&quot;&gt;Control and hallucinations in generated videos:&lt;/h2&gt;
&lt;p&gt;Generative AI that looks nice, I’m wondering if the camera turns multiple laps around the guy in the forest, if the color of the jacket on the back would stay the same without having it explicitly described. What do you think, what would happen? 📹🧥&lt;/p&gt;
&lt;h2 id=&quot;importance-of-genai-control&quot;&gt;Importance of GenAI control:&lt;/h2&gt;
&lt;p&gt;I think overall the opportunity to control generations by precise prompts or other interfaces to further specify intermediate results will be insanely important if these technologies should be adapted in a high efficient outcome-oriented business. We cannot waste time for prompts and regenerates! Cause of time and energy 🌱⏰&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_ai-runway-3d-activity-7259125095783182336-3bZr?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Choosing the Right LLM for Your Needs - Key Considerations</title>
    <link href="https://carstendraschner.github.io/blog/241008_choosing_the_right_llm_for_your_needs_key_considerations/" />
    <updated>2024-10-08T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241008_choosing_the_right_llm_for_your_needs_key_considerations/</id>
    <content type="html">&lt;p&gt;Consider the key factors when selecting a Large Language Model (LLM) for your project.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/TSnOnDgQhc-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/TSnOnDgQhc-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/TSnOnDgQhc-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Benchmark Performance&lt;/li&gt;
&lt;li&gt;License&lt;/li&gt;
&lt;li&gt;Model Size&lt;/li&gt;
&lt;li&gt;Alignment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;key-considerations&quot;&gt;Key Considerations:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;📊 &lt;strong&gt;Benchmark Performance:&lt;/strong&gt; Consider the model&#39;s performance on relevant benchmarks known from Open LLM Leaderboard and especially Arena Elo. &lt;a href=&quot;https://lnkd.in/eSkeAUV7&quot;&gt;https://lnkd.in/eSkeAUV7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📜 &lt;strong&gt;License:&lt;/strong&gt; Ensure the license aligns with your project&#39;s requirements and complies with any regulatory restrictions. &lt;a href=&quot;https://lnkd.in/e8V-eMCh&quot;&gt;https://lnkd.in/e8V-eMCh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📊 &lt;strong&gt;Model Size:&lt;/strong&gt; Consider the trade-off between model size and performance for your specific use case. &lt;a href=&quot;https://lnkd.in/egZt7BmJ&quot;&gt;https://lnkd.in/egZt7BmJ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🔄 &lt;strong&gt;Alignment:&lt;/strong&gt; Evaluate the alignment process and whether it&#39;s transparent, as this can impact the model&#39;s performance and reliability. &lt;a href=&quot;https://lnkd.in/eViiEyqp&quot;&gt;https://lnkd.in/eViiEyqp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🔍 &lt;strong&gt;Transparency of Training:&lt;/strong&gt; Look for models with transparent training data and methods to ensure you understand how the model was trained.&lt;/li&gt;
&lt;li&gt;💸 &lt;strong&gt;Inference Costs:&lt;/strong&gt; Assess the model&#39;s inference costs and consider the trade-off between performance and cost. &lt;a href=&quot;https://lnkd.in/etSajZZc&quot;&gt;https://lnkd.in/etSajZZc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;strong&gt;Context Size:&lt;/strong&gt; Consider the model&#39;s context size and whether it&#39;s suitable for your specific use case.&lt;/li&gt;
&lt;li&gt;🤝 &lt;strong&gt;Compatibility:&lt;/strong&gt; Evaluate whether the model is compatible with SOTA libraries like transformers and whether it&#39;s easily integrable into your workflow.&lt;/li&gt;
&lt;li&gt;📈 &lt;strong&gt;Scaling Efficiency:&lt;/strong&gt; Assess the model&#39;s scaling efficiency or e.g. it has full quadratic complexity with more input tokens.&lt;/li&gt;
&lt;li&gt;🌎 &lt;strong&gt;Multilingualism:&lt;/strong&gt; Evaluate the model&#39;s multilingual capabilities it&#39;s effect for your specific use case. &lt;a href=&quot;https://lnkd.in/eeVsG99M&quot;&gt;https://lnkd.in/eeVsG99M&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-do-you-think&quot;&gt;What Do You Think?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is your importance-order of LLM features you look at? Which criteria do you miss in this list or which should be ranked higher?&lt;/li&gt;
&lt;li&gt;Within our Projects Comma Soft AG these are some of the major criteria we look at when we are selecting GenAI models like LLMs.&lt;/li&gt;
&lt;li&gt;If you like to see more of best practice content, follow me, share your thoughts, and leave me a like ❤️&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#LostInGenai #artificialintelligence #selectllm&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Stop adding Languages to LLMs! The Potential Drawbacks of Training Multilingual Large Language Models (LLMs) for Performance and Sustainability!</title>
    <link href="https://carstendraschner.github.io/blog/241001_stop_adding_languages_to_llms_the_potential/" />
    <updated>2024-10-01T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/241001_stop_adding_languages_to_llms_the_potential/</id>
    <content type="html">&lt;p&gt;Exploring the downsides of creating multilingual LLMs and their impact on performance and resource utilization.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/JA9fsEHeRy-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/JA9fsEHeRy-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/JA9fsEHeRy-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Challenges of building multilingual LLMs&lt;/li&gt;
&lt;li&gt;Inefficiencies in token usage and context length&lt;/li&gt;
&lt;li&gt;Increased hardware costs and reduced token training&lt;/li&gt;
&lt;li&gt;Weighing multilingual models against language-specific models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;building-a-multi-lang-llm&quot;&gt;Building a Multi-Lang-LLM 🛠️🐣&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;When pretraining LLMs, one of the key decisions is which data to include.&lt;/li&gt;
&lt;li&gt;This choice also influences the selection of the tokenizer, which optimizes the number of tokens for the texts used.&lt;/li&gt;
&lt;li&gt;As a result, different characters and character sequences are mapped in the tokenizers for each language.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;when-you-finally-use-such-a-llm-in-only-a-subset-of-available-languages-you-face-following-problems&quot;&gt;When you finally use such a LLM in only a subset of available languages you face following problems 🇪🇺🤖&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Inefficient token usage: If the model is only used for one or two languages, many tokens may be rarely or never needed, leading to shorter token sequences in the required language.&lt;/li&gt;
&lt;li&gt;Limited context length: LLMs have a limited context length, measured in tokens, which can result in more expensive inference as the model scales linearly to quadratically with prompt length.&lt;/li&gt;
&lt;li&gt;Increased hardware costs: This can lead to higher hardware costs and omissions.&lt;/li&gt;
&lt;li&gt;Reduced relevant token training: With a multilingual model, fewer relevant tokens and token sequences may have been seen and trained in the required languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-trade-off-multilingual-models-vs-language-specific-models&quot;&gt;The Trade-Off: Multilingual Models vs. Language-Specific Models 💰📊&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We need to weigh the benefits of multilingual models against the potential drawbacks and decide whether to prioritize language coverage or risk wasting resources.&lt;/li&gt;
&lt;li&gt;This is particularly important when dealing with languages that are not closely related, such as those with different character sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;your-opinion&quot;&gt;Your Opinion 🤗&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What do you think?&lt;/li&gt;
&lt;li&gt;Have you ever chosen a model especially with reduced number of languages outside English?&lt;/li&gt;
&lt;li&gt;Some more details about Multi-Lang-GenAI can be found here: &lt;a href=&quot;https://lnkd.in/edgPsdKz&quot;&gt;https://lnkd.in/edgPsdKz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more content, follow me or reach out to me over DM ❤️&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #llm #languages&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7246524467034689537-nZ-f?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Where Science Meets Innovation - My personal Highlights &amp; Insights into the PG 2024! Do you have answers to the open Questions?</title>
    <link href="https://carstendraschner.github.io/blog/240924_where_science_meets_innovation_my_personal/" />
    <updated>2024-09-24T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240924_where_science_meets_innovation_my_personal/</id>
    <content type="html">&lt;p&gt;Highlights and open questions from the Petersberger Gespräche (PG) 2024, covering AI, energy transition, chip technologies, and more.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/vuj-G2c0U--2198.avif 2198w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/vuj-G2c0U--2198.webp 2198w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/vuj-G2c0U--2198.png&quot; alt=&quot;PG 2024 Highlights&quot; width=&quot;2198&quot; height=&quot;952&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI and consciousness discussions&lt;/li&gt;
&lt;li&gt;Energy transition and regulatory challenges&lt;/li&gt;
&lt;li&gt;Distributed chip technologies in Europe&lt;/li&gt;
&lt;li&gt;Generative AI in media&lt;/li&gt;
&lt;li&gt;Metaverse applications beyond gaming&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;ai-and-consciousness&quot;&gt;🧠 AI and Consciousness 🧠&lt;/h2&gt;
&lt;p&gt;Joscha Bach from Liquid AI presented on how advancements in AI could potentially redefine our understanding of consciousness. My open question for him is: How does he intend to reliably measure the achievement of AI consciousness in his efforts?&lt;/p&gt;
&lt;h2 id=&quot;scientific-chemical-hands-on-how-to-fix-energy-transition&quot;&gt;👩🏻‍🔬 Scientific Chemical Hands on how to fix energy transition 👩🏻‍🔬&lt;/h2&gt;
&lt;p&gt;As chairman of Alexander von Humboldt Foundation, Robert Schlögl delivered an impressively passionate presentation on energy tech for the climate transition and the phasing out of fossil fuels. Besides scientific derivations, he also highlighted significant regulatory issues that hinder successful implementation. I wonder: How these regulations came about, why they are justified if they are (potentially) not scientifically tenable, and how they can now be resolved?&lt;/p&gt;
&lt;h2 id=&quot;efficient-distributed-chip-technologies-from-the-heart-of-europe&quot;&gt;🇪🇺 Efficient Distributed Chip Technologies from the heart of Europe 🇪🇺&lt;/h2&gt;
&lt;p&gt;Christian Mayr from Technische Universität Dresden discussed the potential and developments possible in Dresden. He mentioned the use of clustered chip technologies to operate LLMs. One of my open questions is: How can LLMs be distributed across tens of thousands of chips while still achieving acceptable inferential latencies?&lt;/p&gt;
&lt;h2 id=&quot;genai-in-media-opportunities-and-risks&quot;&gt;📰 GenAI in Media, Opportunities and Risks 📰&lt;/h2&gt;
&lt;p&gt;In further discussions with Sibylle Anderl from ZEIT Verlagsgruppe, we explored the potentials and risks of Generative AI in text, image, and video generation, and how reducing anonymity could potentially restore credibility. My open question: Whether there is a reliable way to recognize these outputs, given that word frequency alone might not be a proof?&lt;/p&gt;
&lt;h2 id=&quot;metaverse-not-only-a-playground&quot;&gt;🕹️ Metaverse, not only a Playground? 🕹️&lt;/h2&gt;
&lt;p&gt;Nico Michels from Siemens presented the potentials of the Metaverse and digital twins. Q: I&#39;d love to see how such models can help even small municipalities with climate adaptation models, regional impact assessments, and improvement options?&lt;/p&gt;
&lt;p&gt;The open questions highlight the stimulating topics the diverse guests who aim to drive positive change with a progressive mindset. I would like to thank Comma Soft AG and everyone involved for organizing this fantastic event, especially Stephan Huthmacher, whose heartfelt dedication makes this recurring, inspiring event possible. ❤️&lt;/p&gt;
&lt;p&gt;Share your thoughts and impressions, I&#39;ll share the streams 🤗&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #sustainability #petersbergergespräche&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-sustainability-activity-7243978164564111361-12R6?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Today&#39;s Research Proposal - How to achieve &quot;real&quot; thinking and reasoning in GenAI, rather than just relying on a silent Chain of Thought, as seen in ReflectionAI or possibly GPT-o1?</title>
    <link href="https://carstendraschner.github.io/blog/240917_todays_research_proposal_how_to_achieve_real/" />
    <updated>2024-09-17T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240917_todays_research_proposal_how_to_achieve_real/</id>
    <content type="html">&lt;p&gt;Exploring the potential for achieving true reasoning and thinking in Generative AI models beyond the current Chain of Thought methodologies.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/W1uRDiZMha-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/W1uRDiZMha-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/W1uRDiZMha-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;696&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current state of reasoning in models&lt;/li&gt;
&lt;li&gt;Possibilities for transformers to learn to think&lt;/li&gt;
&lt;li&gt;Customization ideas for achieving true reasoning&lt;/li&gt;
&lt;li&gt;Open questions and discussion points&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;currently-reasoning-more-through-silent-the-cot&quot;&gt;Currently, reasoning more through silent the CoT 😥&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are currently models that claim to possess reasoning capabilities.&lt;/li&gt;
&lt;li&gt;However, this is more about the Chain of Thought and the use of special tokens.&lt;/li&gt;
&lt;li&gt;These enable the model to generate more text, thereby increasing the stability of its answers.&lt;/li&gt;
&lt;li&gt;This process of &amp;quot;reasoning&amp;quot; is not transparently/barely shown.&lt;/li&gt;
&lt;li&gt;I believe this is less about reasoning or thinking and more about statistical stability.&lt;/li&gt;
&lt;li&gt;Moreover, it is not possible to observe the reasoning process, and it is often difficult to identify when a model has gone astray.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;how-could-transformers-learn-to-think&quot;&gt;How could transformers learn to think? 🤯&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why can I still imagine that it is possible to learn to think or reason with the transformer architecture?&lt;/li&gt;
&lt;li&gt;Due to the transformer architecture, which includes tokenization, attention, and multi-layer perceptron elements, as well as the universal function approximation hypothesis, the following can be imagined:
&lt;ol&gt;
&lt;li&gt;In early layers, the network learns something akin to named entity recognition.&lt;/li&gt;
&lt;li&gt;Later layers and embedding dimensions create structures that learn propositional logic and structures similar to knowledge graphs or their embeddings.&lt;/li&gt;
&lt;li&gt;Finally, based on this, resonating and reflecting completeness of output logic components could be achieved.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-else-would-i-customize-imho&quot;&gt;What else would I customize? 👨🏼‍💻 (IMHO)&lt;/h2&gt;
&lt;p&gt;I would find it interesting to combine this with thought-diffusion models, making hierarchical planning possible. I have several more ideas..., reach out to me if you like to start a discussion ❤️&lt;/p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions:&lt;/h2&gt;
&lt;p&gt;Are you aware of any approaches or research projects that attempt this?
What are your thoughts on that, how would you build &amp;quot;AGI&amp;quot; it?&lt;/p&gt;
&lt;p&gt;I had great idea exchanges on this with colleagues from Comma Soft AG, Lamarr-Institut &amp;amp; Smart Data Analytics.&lt;/p&gt;
&lt;p&gt;Let&#39;s not waste money and time on believing in marketing claims and rebranding of the word &amp;quot;reasoning&amp;quot;, but let&#39;s start to think of how it could actually be achieved 🤗&lt;/p&gt;
&lt;p&gt;#AGI #GenAI #artificialintelligence #research&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-sustainability-activity-7243978164564111361-12R6?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>For more sustainability transparency in GenAI! Share your knowledge and reduce energy waste!</title>
    <link href="https://carstendraschner.github.io/blog/240910_for_more_sustainability_transparency_in_genai_share/" />
    <updated>2024-09-10T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240910_for_more_sustainability_transparency_in_genai_share/</id>
    <content type="html">&lt;p&gt;Emphasizing the importance of transparency and shared knowledge to enhance sustainability in GenAI.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/6P9nHx-165-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/6P9nHx-165-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/6P9nHx-165-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;538&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GenAI involves very large models and significant training efforts&lt;/li&gt;
&lt;li&gt;Transparency can help share emissions and reduce energy waste&lt;/li&gt;
&lt;li&gt;Open source models can optimize future development&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;genai-and-sustainability&quot;&gt;GenAI and Sustainability 🌱&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GenAI is implemented through very large models.&lt;/li&gt;
&lt;li&gt;The training can be substantial, such as the 15T tokens of LLAMA 3.1.&lt;/li&gt;
&lt;li&gt;However, these trainings represent only the final training and not all the attempts that led to the final model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;how-transparency-helps&quot;&gt;How Transparency helps 📚&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If the models are open or, in the best case, open source, many users and use cases can share the resulting emissions.&lt;/li&gt;
&lt;li&gt;It would also be interesting to see how often models are used for inference to compare the relative share of emissions.&lt;/li&gt;
&lt;li&gt;Open source would be important to share learnings for the next generation of models and thus reduce emissions by reusing hyperparameter optimizations.
Please share your findings and let&#39;s use our resources and energy wisely.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;our-paper&quot;&gt;Our Paper 📄&lt;/h2&gt;
&lt;p&gt;In our paper “Ethical and Sustainability Considerations for Knowledge Graph based Machine Learning,” we highlight which sustainability and ethical optimizations are possible when working with hardware-intensive Artificial Intelligence approaches. Link: &lt;a href=&quot;https://lnkd.in/eJjATkTQ&quot;&gt;https://lnkd.in/eJjATkTQ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What are you doing to bring AI, ethics, and sustainability together? ❤️&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #llm #sustainability #aiethics&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_environmental-impact-of-ai-some-model-activity-7236647589096361986-9rSm?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Sustainable Air-Gapped On-Prem LLM Solution! How can we make GenAI available on almost any hardware, and how is it also available as a portable demo on our Alan Notebook</title>
    <link href="https://carstendraschner.github.io/blog/240903_sustainable_air_gapped_on_prem_llm_solution/" />
    <updated>2024-09-03T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240903_sustainable_air_gapped_on_prem_llm_solution/</id>
    <content type="html">&lt;p&gt;Exploring the development of a full-stack GenAI LLM solution that can run on a variety of hardware configurations, including a portable demo setup.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/rVdiXLZ0_K-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/rVdiXLZ0_K-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/rVdiXLZ0_K-800.jpeg&quot; alt=&quot;Alan Notebook&quot; width=&quot;800&quot; height=&quot;879&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Developing Alan, a full-stack GenAI LLM solution&lt;/li&gt;
&lt;li&gt;Hosted on German hyperscaler infrastructure&lt;/li&gt;
&lt;li&gt;Offers a smaller version, Alan-S-LLM&lt;/li&gt;
&lt;li&gt;Portable demo available on Alan Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-we-re-working-on-when-we-talk-about-alan-r-and-d&quot;&gt;What We&#39;re Working on When We Talk About Alan R&amp;amp;D 👨🏼‍💻&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In a recent post, I introduced how we&#39;re developing Alan, a full-stack GenAI LLM solution.&lt;/li&gt;
&lt;li&gt;We host our solution within German hyperscaler infrastructure to deal with the requirements of multiple customer tenants and our large language models, including retrieval augmented generation pipelines.&lt;/li&gt;
&lt;li&gt;The requirements of our strongest Alan LLM require current top-notch Nvidia GPUs (Ampere+, 80GB VRAM), but we also offer a smaller Alan-S-LLM, which still has tremendous capabilities with fewer hardware requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;how-to-shrink-llms&quot;&gt;How to Shrink LLMs 🤖&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Models are smaller in dimensions like the number of transformer layers, heads, hidden dimensions, and other hyperparameters.&lt;/li&gt;
&lt;li&gt;Current smaller GenAI LLMs can be designed by model distillation and model pruning, which try to keep model quality high while reducing the number of parameters.&lt;/li&gt;
&lt;li&gt;The reduced number of parameters reduces VRAM requirements. Fewer parameters, especially fewer transformer layers, increase the throughput and inference performance as well.&lt;/li&gt;
&lt;li&gt;The reduction of bits used to represent each parameter of the LLM reduces the required total GPU VRAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;our-alan-demo-notebook&quot;&gt;Our Alan Demo Notebook 💻&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To demonstrate that our entire tech stack is capable of running entirely air-gapped and to showcase that we&#39;re truly capable of showing this on even a portable system, we developed the Alan-Notebook.&lt;/li&gt;
&lt;li&gt;This notebook uses the entire tech stack, which includes all the components that offer Multi-GPU Cluster setups, handling of users, RAG pipelines, and, of course, LLM text inference.&lt;/li&gt;
&lt;li&gt;The model behind is our fastest and most efficient Alan-S-Model. The notebook has limited hardware capabilities, especially within the GPU (16GB, Nvidia), but can still run the entire tech stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;LLM Model Sizes: &lt;a href=&quot;https://shorturl.at/6ZxMq&quot;&gt;https://shorturl.at/6ZxMq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Alan - Our Developer Journey: &lt;a href=&quot;https://shorturl.at/PqLdE&quot;&gt;https://shorturl.at/PqLdE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full details of how infrastructure is scaled down by Dr. Laura Maaßen see &lt;a href=&quot;https://lmy.de/eicOw&quot;&gt;https://lmy.de/eicOw&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&#39;re interested in how we develop an entire scalable GenAI solution and you want to see some details into our R&amp;amp;D, follow me on LinkedIn and reach out to us. Thanks to my wonderful teammates Dr. Laura Maaßen and Lars Flöer, who made this Alan Notebook possible, and to the entire Alan Development Team Comma Soft AG for supporting this great project and product.&lt;/p&gt;
&lt;p&gt;#genai #onprem #llm #alan&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_genai-onprem-llm-activity-7233851548349444097-lF4N?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Combining the Hugging Face Model Platform and Knowledge Graph trend analysis over time could improve GenAI research and reduce waste of energy!</title>
    <link href="https://carstendraschner.github.io/blog/240830_combining_the_hugging_face_model_platform_and_knowledge/" />
    <updated>2024-08-30T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240830_combining_the_hugging_face_model_platform_and_knowledge/</id>
    <content type="html">&lt;p&gt;Exploring the potential of leveraging knowledge graphs to analyze trends in evolving models for better GenAI research and efficiency.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/Z0Z_6VvJDf-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/Z0Z_6VvJDf-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/Z0Z_6VvJDf-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;545&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leveraging knowledge graphs for GenAI trends&lt;/li&gt;
&lt;li&gt;Identifying high-performing models and best practices&lt;/li&gt;
&lt;li&gt;Potential for a crowd-sourced GenAI cookbook&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traversing the possible huge knowledge graph of evolving models... It would be interesting to see trends within this graph, such as which families and architectures are trending 📈. I&#39;d love to explore over time what approaches for fine-tuning, datasets, or even the number of transformer layers/heads, etc. create high-performing and efficient models 🌱. So we could create a crowd-sourced best practice GenAI cookbook ❤️👩🏽‍🍳. Please let me know if you are already working on it Thomas Wolf 🤗&lt;/p&gt;
&lt;p&gt;#LostInGenai #artificialintelligence #huggingface #knowledgegraphs #genairesearch&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_have-you-noticed-the-new-model-tree-section-activity-7229418184590737409-UoNB?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>What is the perfect approach to adjust an LLM to your GenAI use case?</title>
    <link href="https://carstendraschner.github.io/blog/240729_what_is_the_perfect_approach_to_adjust_an_llm_to_your_genai_use_case/" />
    <updated>2024-07-29T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240729_what_is_the_perfect_approach_to_adjust_an_llm_to_your_genai_use_case/</id>
    <content type="html">&lt;p&gt;Exploring various methods to customize LLMs for specific GenAI use cases, ranging from simple to complex approaches.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/P0lj_EZjfb-906.avif 906w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/P0lj_EZjfb-906.webp 906w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/P0lj_EZjfb-906.png&quot; alt=&quot;Model Training&quot; width=&quot;906&quot; height=&quot;335&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Various ways to customize LLMs for specific use cases&lt;/li&gt;
&lt;li&gt;Approaches vary in difficulty and complexity&lt;/li&gt;
&lt;li&gt;Pros and cons of different methods&lt;/li&gt;
&lt;li&gt;More dimensions to improve GenAI use cases&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;key-considerations&quot;&gt;Key Considerations:&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Prompt Engineering &amp;amp; In-Context Learning:&lt;/strong&gt; Manually enriching the prompt with information to guide the model’s prediction into desired behavior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Generation:&lt;/strong&gt; Automatically retrieving context that is appended to the prompt on the fly after being retrieved through vector database similarity search and reranking.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Parameter-Efficient Finetuning:&lt;/strong&gt; Training only a subset of the model&#39;s parameters while keeping others unchanged.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Full Parameter Finetuning:&lt;/strong&gt; Training all parameters.&lt;/p&gt;
&lt;p&gt;These approaches have pros and cons in terms of (and not limited to):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Catastrophic Forgetting&lt;/li&gt;
&lt;li&gt;Memory Load&lt;/li&gt;
&lt;li&gt;Maximum Context-Size limits&lt;/li&gt;
&lt;li&gt;Hallucinations&lt;/li&gt;
&lt;li&gt;Response Time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also more approaches and dimensions to improve GenAI Use Cases like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guardrails&lt;/li&gt;
&lt;li&gt;Structured output
and many more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you working on, and which field are you interested in to see how we are working on it?&lt;/p&gt;
&lt;p&gt;We at Comma Soft AG implement these and more approaches into our own Product Alan.de and support also other GenAI Use Cases through these approaches.&lt;/p&gt;
&lt;p&gt;Reach out to me 🤗 and follow me for more content ❤️&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #llm&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7228755735776546817-cUi0?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>These results give me hope for sustainable AI 🌱</title>
    <link href="https://carstendraschner.github.io/blog/240722_these_results_give_me_hope_for_sustainable_ai/" />
    <updated>2024-07-22T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240722_these_results_give_me_hope_for_sustainable_ai/</id>
    <content type="html">&lt;p&gt;I&#39;m impressed by some of the recent advances in the field of &amp;quot;small&amp;quot; open-weight Language Models (LLMs).&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/huHTF98SzS-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/huHTF98SzS-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/huHTF98SzS-800.jpeg&quot; alt=&quot;Sustainable AI&quot; width=&quot;800&quot; height=&quot;450&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Increased documentation supports reproducibility&lt;/li&gt;
&lt;li&gt;Data quality improves model performance&lt;/li&gt;
&lt;li&gt;Model distillation reduces hardware needs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;more-documentation&quot;&gt;More Documentation 📚&lt;/h2&gt;
&lt;p&gt;They&#39;re accompanied by increased documentation, as seen with efforts from Apple and Meta, which support reproducibility and reduce wasted effort and energy on less promising pre-training and fine-tuning iterations.&lt;/p&gt;
&lt;h2 id=&quot;data-quality-as-chance&quot;&gt;Data Quality as Chance 🔎&lt;/h2&gt;
&lt;p&gt;They demonstrate that a focus on data quality can improve model performance, as evidenced by Phi-3. However, it&#39;s also clear that the total number of tokens contributes to improvements, as seen with LLAMA3(.1).&lt;/p&gt;
&lt;h2 id=&quot;model-distillation&quot;&gt;Model Distillation 👩🏽‍🔬&lt;/h2&gt;
&lt;p&gt;Model Distillation has been effective in reducing total hardware requirements and inference time, which saves energy and resources. It will be interesting to see how quickly distilled models like Gemma2 can outperform 6-month-old state-of-the-art models like LLAMA2.&lt;/p&gt;
&lt;p&gt;I appreciate the efforts to decrease hardware and energy demands while still providing helpful model responses.&lt;/p&gt;
&lt;p&gt;#artificialintelligence #genai #sustainableai #llm&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_ai-google-tech-activity-7224785383237001216-HDyZ?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>LLMs - Big vs Small. Bigger is Better!? OR Let&#39;s not waste energy!?</title>
    <link href="https://carstendraschner.github.io/blog/240715_llms_big_vs_small_bigger_is_better_or_lets_not_waste_energy/" />
    <updated>2024-07-15T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240715_llms_big_vs_small_bigger_is_better_or_lets_not_waste_energy/</id>
    <content type="html">&lt;p&gt;The AI community is abuzz with debates over the efficacy of large versus small language models. Both have their own merits and limitations.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/nMmqfr9sVq-907.avif 907w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/nMmqfr9sVq-907.webp 907w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/nMmqfr9sVq-907.png&quot; alt=&quot;Model Size&quot; width=&quot;907&quot; height=&quot;337&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI community debates model sizes&lt;/li&gt;
&lt;li&gt;Massive models vs. smaller, efficient models&lt;/li&gt;
&lt;li&gt;Insights and future predictions&lt;/li&gt;
&lt;li&gt;Links to further reading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-ai-community-is-buzzing-with-discussions-about-model-sizes&quot;&gt;✨ The AI community is buzzing with discussions about model sizes:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Massive models like Mistral (8x22B), LLAMA3 (400B), and Grok (314B) are turning heads.&lt;/li&gt;
&lt;li&gt;Smaller yet mighty models like Phi 3, LLAMA3 (8B), and Nemo (12B) are proving their worth.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;key-insights&quot;&gt;🔍 Key insights:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Smaller models can compete by focusing on smarter training and pre-tuning methods.&lt;/li&gt;
&lt;li&gt;Benchmarks are helpful but not flawless in measuring a model&#39;s value.&lt;/li&gt;
&lt;li&gt;The mystery remains: why might a model with 61 transformer layers outperform one with 60?&lt;/li&gt;
&lt;li&gt;Balancing model architecture is crucial due to the increasing complexity as models scale up.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;future-predictions&quot;&gt;🔮 Future predictions:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Will we see a standard model size emerge, or will there be a variety for different model size clusters for different platforms (mobile, single GPU, multi-GPU clusters)?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;links&quot;&gt;Links 🔗&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reason LLAMA Model Sizes: &lt;a href=&quot;https://lnkd.in/dHRSJXgm&quot;&gt;https://lnkd.in/dHRSJXgm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ALAN - GER-hosted GenAI tool developer story: &lt;a href=&quot;https://lnkd.in/ey8aZTjB&quot;&gt;https://lnkd.in/ey8aZTjB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Problems with benchmarks: &lt;a href=&quot;https://lnkd.in/dmBeQZ_j&quot;&gt;https://lnkd.in/dmBeQZ_j&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-is-your-experience&quot;&gt;🌐 What is your experience?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Which model size do you find most practical for real-world problems?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;i-m-keen-to-learn-from-you&quot;&gt;📊 I&#39;m keen to learn from you:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What size models are you working with?&lt;/li&gt;
&lt;li&gt;Share your preferences in this survey and let&#39;s discuss the optimal balance in model scaling.&lt;/li&gt;
&lt;li&gt;What do you think how big is the gpt4o turbo?&lt;/li&gt;
&lt;li&gt;Join the conversation and let&#39;s navigate the evolving landscape of machine learning together ❤️&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Alan (by Comma Soft AG), we recognize the need for variety:
We provide models of various sizes to align with your unique requirements for capability and speed.&lt;/p&gt;
&lt;p&gt;#MachineLearning #AI #ModelSize #Innovation #GenAI #SustainableAI&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_machinelearning-ai-modelsize-activity-7221169255419969536-IQnL?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>GenAI, what is plagiarism? Its Impact on Science. How should it be handled? What is your perspective?</title>
    <link href="https://carstendraschner.github.io/blog/240708_genai_what_is_plagiarism_its_impact/" />
    <updated>2024-07-08T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240708_genai_what_is_plagiarism_its_impact/</id>
    <content type="html">&lt;p&gt;Discussing the implications of GenAI on scientific work and the thin line between acceptable use and plagiarism.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/PLpWNuwzop-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/PLpWNuwzop-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/PLpWNuwzop-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use of GenAI in scientific work&lt;/li&gt;
&lt;li&gt;Acceptable vs. debatable vs. critical usage&lt;/li&gt;
&lt;li&gt;Questions and concerns about plagiarism&lt;/li&gt;
&lt;li&gt;The pressure on researchers and students&lt;/li&gt;
&lt;li&gt;Opportunities for better research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;current-situation&quot;&gt;Current Situation 🏫&lt;/h2&gt;
&lt;p&gt;However, I see ChatGPT being used in many researchers&#39; work to assist in the creation of (scientific) texts. Often it is also controlled or prohibited by the controlling institutions (schools, universities, paper-venues) to use such tools.&lt;/p&gt;
&lt;h2 id=&quot;sample-genai-llm-usage-i-think-is-ok-imho&quot;&gt;Sample GenAI-LLM usage I think is ok (IMHO) 🤗&lt;/h2&gt;
&lt;p&gt;I wonder how the following behavior in ChatGPT should be dealt with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You ask Chat GPT if it would correct commas and or capitalization in an existing text as Office Suites do&lt;/li&gt;
&lt;li&gt;You ask for rewording from passive to active or better and more varied synonyms such as in Grammarly which is also &amp;quot;GenAI&amp;quot;&lt;/li&gt;
&lt;li&gt;You can have texts translated to make quotes more easily accessible in the desired language like DeepL or Google Translate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all uses of Chat-GPT that I can imagine in the context of scientific work that was previously possible without &amp;quot;ChatGPT&amp;quot; and, in my opinion, doable with other tools without having to speak of plagiarism.&lt;/p&gt;
&lt;h2 id=&quot;debatable-usage&quot;&gt;Debatable usage 🤷🏼‍♂️&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You are generating the abstract for your finished paper while correcting and polishing it manually yourself afterwards.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;more-critical-usage&quot;&gt;More critical usage 🧐&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Write an entire text, paper/exercise/... simply based on a very refined prompt with LLM+RAG+Response-Verification and hand it in as your own work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;my-questions&quot;&gt;My Questions 👨🏼‍🎓&lt;/h2&gt;
&lt;p&gt;I just ask myself how I would have worked if the tools were there or how I would want to deal with these kinds of situations as a lecturer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What should be allowed?&lt;/li&gt;
&lt;li&gt;What goes too far and falls under plagiarism?&lt;/li&gt;
&lt;li&gt;Which use of (GenAI) tools needs to be regulated and documented and how finely tuned?&lt;/li&gt;
&lt;li&gt;How do you think scientific work, reviews, and publications will change for better or for worse?&lt;/li&gt;
&lt;li&gt;Does your institution at least clearly regulate how (GenAI-)Tool usage is possible&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;current-problem&quot;&gt;Current Problem 🫣&lt;/h2&gt;
&lt;p&gt;I see lots of pressure on students and research scientists to hand in their work or to publish papers. So they are tempted to use available tools. Not every GenAI will imply plagiarism but regulations are not clear as far as I have seen.&lt;/p&gt;
&lt;h2 id=&quot;the-chance-to-research&quot;&gt;The Chance to Research 👨🏼‍🔬&lt;/h2&gt;
&lt;p&gt;We have the chance to use GenAI for better research: acquire good information over RAG and Vector Search, Translate texts into our mother tongue, and Rewrite in shorter or clearer language or even more accessible language.... Chat interact with papers you have questions to....&lt;/p&gt;
&lt;p&gt;We at Comma Soft AG develop GenAI solutions and also actively using such tools to create great solutions while being aware and careful about possible plagiarism issues.&lt;/p&gt;
&lt;p&gt;#genai #plagiarism #research #artificialintelligence&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_genai-plagiarism-research-activity-7216090153457463298-37OX?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Adjust GenAI responses towards more ethical behavior possible through system prompts!? Do you trust in such LLM-chat prepended pamphlets?</title>
    <link href="https://carstendraschner.github.io/blog/240701_adjust_genai_responses_towards_more_ethical_behavior_possible/" />
    <updated>2024-07-01T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240701_adjust_genai_responses_towards_more_ethical_behavior_possible/</id>
    <content type="html">&lt;p&gt;Exploring the potential and challenges of using system prompts to guide LLM behavior towards ethical outputs.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/x7ZX3a4wEx-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/x7ZX3a4wEx-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/x7ZX3a4wEx-800.jpeg&quot; alt=&quot;Ethical AI&quot; width=&quot;800&quot; height=&quot;800&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GenAI chat interactions often include system prompts&lt;/li&gt;
&lt;li&gt;System prompts aim to guide ethical LLM behavior&lt;/li&gt;
&lt;li&gt;Challenges exist in ensuring compliance and formulation&lt;/li&gt;
&lt;li&gt;Questions on designing and revealing system prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;background&quot;&gt;Background 🤓&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GenAI is often used via chat&lt;/li&gt;
&lt;li&gt;We enter prompts in the Chat UI&lt;/li&gt;
&lt;li&gt;Under the hood, the Chat programs usually add a system prompt&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;system-prompt&quot;&gt;System-Prompt 💬&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This system prompt should control the behavior of the LLM output&lt;/li&gt;
&lt;li&gt;The system prompt complements other behavioral optimizations such as alignment and other guardrails to improve ethical GenAI response&lt;/li&gt;
&lt;li&gt;The system prompt can describe behavior such as identity or desired behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;problem&quot;&gt;Problem 🥴&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There is no guarantee to what extent the system prompt will be followed&lt;/li&gt;
&lt;li&gt;It is unclear what details should be included in the system prompt&lt;/li&gt;
&lt;li&gt;It is not possible to prevent the model from revealing the system prompt&lt;/li&gt;
&lt;li&gt;It is unclear how best to formulate the system prompt&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions 🤔&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How do you design system prompts, and what are yours?&lt;/li&gt;
&lt;li&gt;Do you think system prompts help to ensure the model&#39;s behavior?&lt;/li&gt;
&lt;li&gt;Do you think chat UIs should not reveal the system prompt and how would you achieve this?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading 📖&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Change Alignment of LLMs: &lt;a href=&quot;https://lnkd.in/eWS-VZCD&quot;&gt;https://lnkd.in/eWS-VZCD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LLM Behavior: &lt;a href=&quot;https://lnkd.in/ed52RBNe&quot;&gt;https://lnkd.in/ed52RBNe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My current LLM Developments: &lt;a href=&quot;https://lnkd.in/ey8aZTjB&quot;&gt;https://lnkd.in/ey8aZTjB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We at Comma Soft AG develop GenAI solutions including optimizations of LLM responses through various components where also the design of system prompts plays a role.&lt;/p&gt;
&lt;p&gt;#artificialintelligence #responsibleai #llm #alan #aiethics&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-responsibleai-llm-activity-7214141563738750976-ywik?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Alternative to GenAI creativity? Watch and try out these fun Evolutionary Algorithms. Problem-solving without GenAI and SGD-based approaches explained!</title>
    <link href="https://carstendraschner.github.io/blog/240624_alternative_to_genai_creativity_watch_and_try/" />
    <updated>2024-06-24T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240624_alternative_to_genai_creativity_watch_and_try/</id>
    <content type="html">&lt;p&gt;Exploring Evolutionary Algorithms as an alternative to GenAI for problem-solving, using a fun 2D vehicle race example.&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/D6xLKFbtVN-986.avif 986w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/D6xLKFbtVN-986.webp 986w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/D6xLKFbtVN-986.png&quot; alt=&quot;Evo Cars&quot; width=&quot;986&quot; height=&quot;498&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR ⏱️&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is hype around GenAI and LLMs&lt;/li&gt;
&lt;li&gt;Evolutionary Algorithms (EAs) offer an alternative&lt;/li&gt;
&lt;li&gt;A fun example of EAs using a 2D vehicle race&lt;/li&gt;
&lt;li&gt;Steps involved in EAs explained&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;background&quot;&gt;Background ☺️&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Within the field of AI is a GenAI and LLM Hype&lt;/li&gt;
&lt;li&gt;These are not always the best approach to solve a problem!&lt;/li&gt;
&lt;li&gt;There are interesting algorithms optimizing their behavior entirely differently: e.g., Evolutionary Algorithms (EA).&lt;/li&gt;
&lt;li&gt;Today I want to show you a fun EA introduction based on a 2D vehicle race &amp;quot;problem&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;evolutionary-algorithms-eas-explained&quot;&gt;Evolutionary algorithms (EAs) explained 👨🏼‍🏫&lt;/h2&gt;
&lt;h3 id=&quot;problem&quot;&gt;Problem 🏁:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the best vehicle configuration to drive over a 2D surface?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;vehicle-configuration-genome&quot;&gt;Vehicle Configuration = Genome 🧬:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shape&lt;/li&gt;
&lt;li&gt;Wheel size&lt;/li&gt;
&lt;li&gt;Wheel position&lt;/li&gt;
&lt;li&gt;Wheel weight&lt;/li&gt;
&lt;li&gt;Chassis weight&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;key-ea-steps-on-the-vehicle-race-example-1234&quot;&gt;Key EA Steps on the vehicle race example 🔢 :&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: Generate a set of vehicles with random characteristics (genome) called population&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fitness Evaluation&lt;/strong&gt;: Check how far they have got on the randomly generated surface.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selection&lt;/strong&gt;: Select a subset of the best vehicles from the population to reproduce and form the next generation, e.g., by tournament selection or rank selection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crossover (Recombination)&lt;/strong&gt;: Combine the genetic information of two or more selected vehicles to create new offspring. E.g., average two good car genomes or randomly select the information of each...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mutation&lt;/strong&gt;: Randomly modify the genetic information of some individuals in the population to introduce new variations and prevent convergence to a local optimum.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Replacement&lt;/strong&gt;: Replace the least performing vehicles with new offspring generated through crossover and mutation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Termination&lt;/strong&gt;: Repeat until a stopping criterion is met: the needed distance, number of iterations, or convergence threshold.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: best vehicle, representing the near-optimal solution&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;my-perspective&quot;&gt;My Perspective 🤗:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Be aware that there are a multitude of possible algorithms to use&lt;/li&gt;
&lt;li&gt;We at Comma Soft AG do not trust any hype and select the appropriate algorithms for the respective problem&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;credit&quot;&gt;Credit ❤️&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To the development team of the interactive EA vehicle race&lt;/li&gt;
&lt;li&gt;I&#39;d also like to thank Hajira Jabeen and Jens Lehmann for their great support in my MT project where I worked on EA and KG for the creation of novel recipes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;links&quot;&gt;Links 📖&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EA vehicle race, try it out yourself: &lt;a href=&quot;https://lnkd.in/e2Ew3e7D&quot;&gt;https://lnkd.in/e2Ew3e7D&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Extract from my MT in a short paper: &lt;a href=&quot;https://lnkd.in/e8VRVe_k&quot;&gt;https://lnkd.in/e8VRVe_k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My current GenAI work: &lt;a href=&quot;https://lnkd.in/edNx8uKh&quot;&gt;https://lnkd.in/edNx8uKh&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How would you solve this &amp;quot;race&amp;quot; problem? EA, GenAI, RL, ...?&lt;/li&gt;
&lt;li&gt;Do you want to see more non-GenAI content?&lt;/li&gt;
&lt;li&gt;Have you ever used EA to solve a problem?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stay tuned by following me here on Linked ❤️&lt;/p&gt;
&lt;p&gt;#LostInGenai #artificialintelligence #evolutionaryalgorithms&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-evolutionaryalgorithms-activity-7211741750673973248-V4Ej?utm_source=share&amp;amp;utm_medium=member_desktop&quot;&gt;LinkedIn Post&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Fast New ELO Over MixEval. But Looking into Code, I Got Doubts!</title>
    <link href="https://carstendraschner.github.io/blog/240620_MixEval_doubts/" />
    <updated>2024-06-20T00:00:00Z</updated>
    <id>https://carstendraschner.github.io/blog/240620_MixEval_doubts/</id>
    <content type="html">&lt;p&gt;Have you seen that MixEval Hard has two interesting but little critical aspects&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://carstendraschner.github.io/img/Qs-Bzleob6-800.avif 800w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://carstendraschner.github.io/img/Qs-Bzleob6-800.webp 800w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://carstendraschner.github.io/img/Qs-Bzleob6-800.jpeg&quot; alt=&quot;Image 1&quot; width=&quot;800&quot; height=&quot;745&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Examined MixEval, an open-source LLM benchmark&lt;/li&gt;
&lt;li&gt;Uses LLMs as judges to predict continuous values&lt;/li&gt;
&lt;li&gt;Evaluation data contains duplicates&lt;/li&gt;
&lt;li&gt;Uncertain about trusting these methods&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;issues-with-mixeval&quot;&gt;Issues with MixEval&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;For freeform answering, it lets e.g. GPT35T as judge predict values like &amp;quot;[[0.9]]&amp;quot; and try to fetch this as reliable number for correctness score.&lt;/li&gt;
&lt;li&gt;The MixEval eval data contains several duplicates due to their sampling approach.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;my-take&quot;&gt;My Take&lt;/h2&gt;
&lt;p&gt;I like the concept of MixEval to have an open-source LLM benchmark which has a good overlap to Arena Elo, but I am unsure if I trust LLMs as judge which try to reason continuous numbers for freeform LLM answers and relies on duplicates in samples.&lt;/p&gt;
&lt;h2 id=&quot;extract-of-my-hands-on-criteria&quot;&gt;Extract of My Hands-On Criteria&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What do you think?&lt;/li&gt;
&lt;li&gt;Do you see an issue in duplicates within eval sets?&lt;/li&gt;
&lt;li&gt;Would you trust LLMs as judge predicting continuous values for later usage?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;credit&quot;&gt;Credit&lt;/h2&gt;
&lt;p&gt;Thanks Philipp Schmid for your post(s)&lt;/p&gt;
&lt;h2 id=&quot;my-questions&quot;&gt;My Questions?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What are your thoughts on MixEval?&lt;/li&gt;
&lt;li&gt;Do you trust LLMs as judges in evaluating continuous values?&lt;/li&gt;
&lt;li&gt;How do you handle duplicates in evaluation sets?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#generativeai #artificialintelligence #llm #machinelearning #benchmark&lt;/p&gt;
</content>
  </entry>
</feed>