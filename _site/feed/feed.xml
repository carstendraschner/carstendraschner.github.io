<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Blog Title</title>
  <subtitle>This is a longer description about your blog.</subtitle>
  <link href="https://example.com/feed/feed.xml" rel="self" />
  <link href="https://example.com/" />
  <updated>2018-09-30T00:00:00Z</updated>
  <id>https://example.com/</id>
  <author>
    <name>Your Name</name>
  </author>
  <entry>
    <title>How Far Are We Really from Reasoning within GenAI? My Perspective in GPT-o1 times and Possible Ways to Get There</title>
    <link href="https://example.com/blog/reasoning-in-genai/" />
    <updated>2018-09-30T00:00:00Z</updated>
    <id>https://example.com/blog/reasoning-in-genai/</id>
    <content type="html">&lt;p&gt;Exploring the current state of reasoning in GenAI, the challenges faced, and potential approaches to achieving true reasoning capabilities.&lt;/p&gt;
&lt;h2 id=&quot;reasoning-is-a-key-ability-to-transform-ai-into-agi&quot;&gt;Reasoning is a Key Ability to Transform AI into AGI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In Reflection AI, reasoning has been unmasked.&lt;/li&gt;
&lt;li&gt;OpenAIs GPT-o1 behavior could be reproduced by silent chain-of-thought (CoT) with other models.&lt;/li&gt;
&lt;li&gt;Reasoning is still more of a marketing promise than an actual capability.&lt;/li&gt;
&lt;li&gt;For example, we observe significant weaknesses in simple tasks.&lt;/li&gt;
&lt;li&gt;Simple multiplication is still a challenge for many models.&lt;/li&gt;
&lt;li&gt;As long as these fundamental issues remain unresolved, we are far from achieving true reasoning capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;why-i-believe-llms-should-be-able-to-learn-math-and-reasoning&quot;&gt;Why I Believe LLMs Should Be Able to Learn Math and Reasoning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are varying opinions on whether reasoning is achievable with current AI.&lt;/li&gt;
&lt;li&gt;I am fundamentally optimistic, although I acknowledge the recent criticisms from Yann LeCun and older from Emily M. Bender.&lt;/li&gt;
&lt;li&gt;I have outlined my vision for reasoning in the comments section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;how-to-teach-math-skills-as-a-first-step&quot;&gt;How to Teach Math Skills as a First Step&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let&#39;s focus on multiplication as a starting point.&lt;/li&gt;
&lt;li&gt;Generate pairs of numbers of different sizes.&lt;/li&gt;
&lt;li&gt;Split the data into training, testing, and validation sets.&lt;/li&gt;
&lt;li&gt;Train a language model (LLM) using a tokenizer that only includes numbers 0-9, &#39;x&#39; for multiplication, and &#39;EOS&#39; for the end of the sequence.&lt;/li&gt;
&lt;li&gt;Evaluate which model size yields the best results on the test set.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-do-you-think&quot;&gt;What Do You Think?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Could such an experimental setup help evaluate the computational skills of LLMs/transformer architectures?&lt;/li&gt;
&lt;li&gt;Do you agree that tokens representing individual numbers are crucial for computation, rather than embedding multiple digits like &amp;quot;007&amp;quot; in a single token?&lt;/li&gt;
&lt;li&gt;Have any of you tried something similar, or would you like to collaborate on this research project?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more content like this, follow me If you enjoyed this post, please leave a like or share your thoughts and ideas in the comments. Image first seen in a post from Yuntian Deng. Looking forward to discuss this with my lovely colleagues at Comma Soft AG within our next AI-Coffee-Hour&lt;/p&gt;
&lt;p&gt;#GenAI #Reasoning #LLM #SimpleMath&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Choosing the Right LLM for Your Needs - Key Considerations</title>
    <link href="https://example.com/blog/choosing-the-right-llm/" />
    <updated>2018-09-30T00:00:00Z</updated>
    <id>https://example.com/blog/choosing-the-right-llm/</id>
    <content type="html">&lt;p&gt;Consider the key factors when selecting a Large Language Model (LLM) for your project.&lt;/p&gt;
&lt;h2 id=&quot;key-considerations&quot;&gt;Key Considerations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Benchmark Performance&lt;/li&gt;
&lt;li&gt;License&lt;/li&gt;
&lt;li&gt;Model Size&lt;/li&gt;
&lt;li&gt;Alignment&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;benchmark-performance&quot;&gt;Benchmark Performance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consider the model&#39;s performance on relevant benchmarks known from Open LLM Leaderboard and especially Arena Elo.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/eSkeAUV7&quot;&gt;Link to Open LLM Leaderboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;license&quot;&gt;License&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ensure the license aligns with your project&#39;s requirements and complies with any regulatory restrictions.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/e8V-eMCh&quot;&gt;Link to license information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;model-size&quot;&gt;Model Size&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consider the trade-off between model size and performance for your specific use case.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/egZt7BmJ&quot;&gt;Link to model size information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;alignment&quot;&gt;Alignment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Evaluate the alignment process and whether it&#39;s transparent, as this can impact the model&#39;s performance and reliability.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/eViiEyqp&quot;&gt;Link to alignment information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;transparency-of-training&quot;&gt;Transparency of Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Look for models with transparent training data and methods to ensure you understand how the model was trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;inference-costs&quot;&gt;Inference Costs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Assess the model&#39;s inference costs and consider the trade-off between performance and cost.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/etSajZZc&quot;&gt;Link to inference cost information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;context-size&quot;&gt;Context Size&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Consider the model&#39;s context size and whether it&#39;s suitable for your specific use case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;compatibility&quot;&gt;Compatibility&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Evaluate whether the model is compatible with SOTA libraries like transformers and whether it&#39;s easily integrable into your workflow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;scaling-efficiency&quot;&gt;Scaling Efficiency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Assess the model&#39;s scaling efficiency or e.g. it has full quadratic complexity with more input tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;multilingualism&quot;&gt;Multilingualism&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Evaluate the model&#39;s multilingual capabilities it&#39;s effect for your specific use case.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://lnkd.in/eeVsG99M&quot;&gt;Link to multilingualism information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-do-you-think&quot;&gt;What Do You Think?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What is your importance-order of LLM features you look at? Which criteria do you miss in this list or which should be ranked higher?&lt;/li&gt;
&lt;li&gt;Within our Projects Comma Soft AG these are some of the major criteria we look at when we are selecting GenAI models like LLMs.&lt;/li&gt;
&lt;li&gt;If you like to see more of best practice content, follow me, share your thoughts, and leave me a like&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#LostInGenai #artificialintelligence #selectllm&lt;/p&gt;
</content>
  </entry>
</feed>