<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Ever wondered about open source LLLM sizes - 7B, 13B, 70B?</title>
		<meta name="description" content="I am writing about my experiences as a naval navel-gazer.">
		<link rel="alternate" href="feed/feed.xml" type="application/atom+xml" title="Carsten Felix Draschner, PhD">
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
/* This is an arbitrary CSS string added to the bundle */
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;
	--footer-background-color: #333;
	--footer-font-color: #fff;
	--header-background-color: #333;
	--header-font-color: #fff;
	--footer-height: 4rem;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #4e5f93;
	--text-color-link-visited: #245242;

	--syntax-tab-size: 2;

	--card-background-color: #f9f9f9;
	--card-border-color: #ccc;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;

		/* Dark mode card colors */
		--card-background-color: #333;
		--card-border-color: #555;
	}
}

.card {
	display: flex;
	align-items: center;
	border: 1px solid var(--card-border-color);
	padding: 10px;
	margin-bottom: 10px;
	border-radius: 8px;
	background-color: var(--card-background-color);
}

.card h3 {
	margin: 0;
  }
  
.card p {
	margin: 0;
}

.card div {
	padding-left: 10px;
}

/* Global stylesheet */
* {
	box-sizing: border-box;
}

@view-transition {
	navigation: auto;
}

html,
body {
	height: 100%;
	padding: 0;
	margin: 0;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
.color-gray-50{
	color:var(--color-gray-50)
}

img {
    max-width: 100%;
    height: auto;
}


/* .h-excerpt{
	font-size: 1em;
} */

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main {
	padding: 1rem;
	flex: 1;
	max-width: 40em;
	margin: 0 auto;
	margin-bottom: var(--footer-height);
}
main :first-child {
	margin-top: 0;
}



footer {
	height: var(--footer-height);
    background: var(--footer-background-color);
    color: var(--footer-font-color);
    text-align: center;
    padding: 10px 0;
    bottom: 0;
    width: 100%;
	p {
		height: 100%;
		display: flex;
		align-items: center;
		justify-content: center;
		margin: 0;
	}
}

.links-nextprev {
	display: flex;
	justify-content: space-between;
	gap: .5em 1em;
	list-style: "";
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}
.links-nextprev > * {
	flex-grow: 1;
}
.links-nextprev-next {
	text-align: right;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	overflow-x: auto;
}
code {
	word-break: break-all;
}

/* Header */
header {
	background: var(--header-background-color);
	color: var(--header-font-color);
	padding: 1rem 0;
	text-align: center;
	a[href] {
		color: var(--header-font-color);
	}
	a[href]:visited {
		color: var(--header-font-color);
	}
}
.site-title {
	font-size: 2em;
	padding-bottom: 0.4em;
}

.container {
	display: flex; /* Hinzugefügt */
	flex-direction: column; /* Hinzugefügt */
	min-height: 100vh; /* Hinzugefügt */
  }

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
    align-items: center;
    flex-direction: row;
    justify-content: center;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

.card {
	display: flex;
	align-items: center;
	border: 1px solid #ccc;
	padding: 10px;
	margin-bottom: 10px;
	border-radius: 8px;
	background-color: var(--background-color);
}
.card img {
	border-radius: 8px;
	height: auto;
	width: 90px;
}
.card .card-title {
	font-size: 1.17em;
	font-weight: bold;
}
.card-navigation ul {
	flex: 1;
	list-style: none;
	padding: 0;
}


@media (prefers-color-scheme: dark) {
	.invert-for-dark-mode {
		filter: invert(100%);
	}
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	margin-top: 1em;
	padding-bottom: 1em;
	margin-bottom: 1em;
	border-bottom: 1px solid var(--color-gray-50);
}
.postlist-item:last-child{
	border-bottom: 0;
}

.postlist-date {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Anchor links next to heading */
.ha-placeholder {
	opacity: 0 !important;
}
h2:has(.ha-placeholder):hover,
h3:has(.ha-placeholder):hover,
h4:has(.ha-placeholder):hover,
h5:has(.ha-placeholder):hover,
h6:has(.ha-placeholder):hover {
	.ha-placeholder {
		opacity: 0.3 !important;
	}
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<div class="site-title">Carsten Felix Draschner, PhD</div>
			<nav>
				<h2 class="visually-hidden" id="top-level-navigation-menu">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/">Home</a></li>
					<li class="nav-item"><a href="/blog/">Blog</a></li>
					<li class="nav-item"><a href="/research/">Research</a></li>
					<li class="nav-item"><a href="/projects/">Projects</a></li>
					<li class="nav-item"><a href="/about/">CV</a></li>
					<li class="nav-item"><a href="/contact/">Contact</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			<heading-anchors>
				
<h1 id="ever-wondered-about-open-source-lllm-sizes-7b-13b-70b">Ever wondered about open source LLLM sizes - 7B, 13B, 70B?</h1>

<ul class="post-metadata">
	<li><time datetime="2024-02-27">27 February 2024</time></li>
	<li><a href="/tags/blog/" class="post-tag">blog</a></li>
</ul>


<p>Where do those model sizes come from?... My findings!</p>
<p><picture><source type="image/avif" srcset="/img/Om6W9Bb879-800.avif 800w"><source type="image/webp" srcset="/img/Om6W9Bb879-800.webp 800w"><img loading="lazy" decoding="async" src="/img/Om6W9Bb879-800.jpeg" alt="Image 1" width="800" height="800"></picture></p>
<p><strong>TL;DR ⏱️</strong></p>
<ul>
<li>Open-source LLM alternatives to AIaaS</li>
<li>Hugging Face as a source for open-source models</li>
<li>Many models are finetuned variants</li>
<li>Bigger models imply slower inference &amp; higher costs</li>
<li>Different use cases require different model capabilities</li>
<li>Questioning the parameter step sizes of models</li>
</ul>
<h2 id="background">Background 📝</h2>
<ul>
<li>As an alternative to AIaaS like ChatGPT, you can interact with LLMs based on open-source models.</li>
<li>A good source for open-source models is the Hugging Face model library.</li>
<li>Many models are finetuned variants of existing models like the Meta LLAMA-2 is available as 7B, 13B, 70B.</li>
<li>Inference runs faster on GPUs like NVIDIA V/A/H100.</li>
<li>Bigger models: slower inference &amp; have higher (environmental) costs while bigger LLMs mostly outperform smaller models in benchmark-tasks.</li>
<li>Within multiple use cases, I select the best fitting OS model at @Comma Soft AG use cases.</li>
<li>Different use cases request different model capabilities including model “knowledge” or inference speed.</li>
<li>I was wondering why many models follow the parameter “step sizes” 7B, 13B, and 70B.</li>
</ul>
<h2 id="my-findings">My Findings 🔍</h2>
<ul>
<li>Many models are finetuned versions of LLAMA-2 as this was a high-performing open-source LLM available within a “mostly” attractive OS license.</li>
<li>In most cases, model finetuning does not change the number of parameters.</li>
<li>LLAMA paper states it provides LLMs: [...] &quot;that achieve the best possible performance at various inference budgets&quot; [...]</li>
<li>Common hardware is 16GB or 80GB of VRAM. Usually, you have one or two of those GPUs within a system.</li>
<li>Models are by default available as 16bit representation which leads to 2byte per parameter.</li>
<li>To run a model, you need space for parameters and a bit remaining for your batch. So 7B fits on 1x 16GB GPU, 13B fits on 2x 16GB GPUs, (the Lab-leaked LLAMA-1 with 35B fits on 1x 80GB GPU) and the 70B LLAMA-2 model runs on 2x 80GB GPUs.</li>
</ul>
<h2 id="imho">IMHO 🤗</h2>
<ul>
<li>I am looking forward to how 4bit quantization like GPTQ or AWQ changes the model sizes as you might also fit a roughly 145B quantized model on a single A100 with 80GB.</li>
<li>Some use cases might need fewer model parameters but bigger batches, longer max context length, or faster inference which means fewer parameters or fewer deep networks.</li>
<li>Consider smaller models especially because of the environmental costs if performance is sufficient.</li>
<li>I am wondering if there is a true reason for how the parameters are combined within the architecture, the numbers partially feel randomly picked like 80 transformer layers for LLAMA-2 70B vs 40 of 13B version.</li>
</ul>
<h2 id="questions">Questions 🤔</h2>
<ul>
<li>What is your preferred model(-family)?</li>
<li>Do you use your models as plain or quantized versions?</li>
<li>Do you think the model architectures of finetuned context window, hidden size, intermediate size, number heads, and transformer layers are well chosen that build the total needed VRAM volume?</li>
</ul>
<p>Follow me for more content ❤️</p>
<p>#artificialintelligence #genai #machinelearning #llm</p>
<p><a href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-maschinelearning-activity-7163192285034172416-AItW?utm_source=share&amp;utm_medium=member_desktop">LinkedIn Post</a></p>

<ul class="links-nextprev"><li class="links-nextprev-prev">← Previous<br> <a href="/blog/240220_beaten_gpt4/">We&#39;ve beaten GPT4! ... is a sentence which starts to annoy me.</a></li><li class="links-nextprev-next">Next →<br><a href="/blog/240307_not_prompting_in_english/">Not prompting in English?... You have Problems!! LLM Language Barriers • Democratizing GenAI and fair pricing</a></li>
</ul>

			</heading-anchors>
		</main>

		<footer>
			<p>&copy; 2024 Carsten Felix Draschner, PhD</p>
		</footer>

		<!-- This page `/blog/240227_ever_wondered_about_open_source_llm_sizes/` was built on 2025-02-10T15:48:03.386Z -->
		<script type="module" src="/dist/rJ3_G-2ArF.js"></script>
	</body>
</html>
